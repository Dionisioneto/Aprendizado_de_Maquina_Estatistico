if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, betareg, betaboost)
setwd("C:/Users/dioni/OneDrive - University of São Paulo/Doutorado em Estatística/2023.2/2 - Aprendizado de Máquina Estatístico/3_Trabalho_Final")
bf = read.csv("https://raw.githubusercontent.com/Dionisioneto/Aprendizado_de_Maquina_Estatistico/main/3_Trabalho_Final/dados/bolsafamilia_tx.csv")
head(bf)
## retirar a coluna indice
bf = bf %>% subset(select=-c(X))
y = bf %>% subset(select=c(tx_benbf))
y = bf %>% subset(select=c(tx_benbf))
X = bf %>% subset(select=c(FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,
GINI, PIND, PPOB, RDPCT,
THEIL, P_FORMAL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
PAREDE, T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R,
tx_ocupacao_urbana, capital))
y[y>0.99999] = 0.9999
databf = cbind(X,y)
dim(databf)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(databf) * prop_treino)
ind_treino = sample(1:nrow(databf), n_treino)
# Criar conjuntos de treino e teste
treinobf = databf[ind_treino, ]
testebf = databf[-ind_treino, ]
dim(treinobf);dim(testebf)
breg = betareg(tx_benbf ~.,
data=treinobf)
ypredbreg = predict(breg,testebf)
MSE = function(ypred,ytrue){sum((ypred - ytrue)^2)/length(ytrue)}
msebeta = MSE(ypred=ypredbreg,ytrue=testebf$tx_benbf)
msebeta
sqrt(msebeta)
betareg?
?betareg
linksfun = c("logit", "probit", "cloglog", "cauchit", "log", "loglog")
for (i in linksfun){prit(i)}
for (i in linksfun){print(i)}
tunebetareg= tune(betareg,tx_benbf ~.,
data = treinobf,
ranges=list(link=linksfun),
tunecontrol = tune.control(cross = 10))
p_load(ggplot2,dplyr, betareg, betaboost,e1071)
tunebetareg= tune(betareg,tx_benbf ~.,
data = treinobf,
ranges=list(link=linksfun),
tunecontrol = tune.control(cross = 10))
tunebetareg = tune(betareg,tx_benbf ~.,
data = treinobf,
ranges=list(link="cloglog"),
tunecontrol = tune.control(cross = 10))
for (link in linksfun) {
print(link)
}
p_load(ggplot2,dplyr, betareg, betaboost,e1071,caret)
# Usando createDataPartition para obter índices de divisão
indices_divisao = createDataPartition(y = treinobf,
times = 10,
p = 0.7,
list = FALSE)
set.seed(10)
# Usando createDataPartition para obter índices de divisão
indices_divisao = createDataPartition(y = treinobf,
times = 10,
p = 0.8,
list = FALSE)
# Usando createDataPartition para obter índices de divisão
indices_divisao = createDataPartition(y = treinobf,
times = 10,
p = 0.8)
treinobf
# Usando createDataPartition para obter índices de divisão
indices_divisao = createDataPartition(y = treinobf,
times = 10,
p = 0.8)
seu_conjunto_de_dados <- 1:10
# Número desejado de divisões
num_divisoes <- 5
# Criando índices de divisão usando cut
indices_divisao <- cut(seq_along(seu_conjunto_de_dados), breaks = num_divisoes, labels = FALSE)
# Criando índices de divisão usando cut
indices_divisao
seq_along(seu_conjunto_de_dados)
library(tidyverse)
library(dplyr)
make_data<-function(n){
x = rnorm(n)
y = 2*x + 1 + rnorm(n, 0, 1)
tibble(x, y)
}
kfolds <- 10
train_n <-1000
train_data <- make_data(train_n) %>%
mutate(fold = sample(1:kfolds, size=train_n, replace=T))
train_data
cv_err = rep(0, kfolds)
for(i in 1:kfolds){
in_data <- filter(train_data, fold!=i)
out_data <- filter(train_data, fold==i)
fit <- lm(y~x, data = in_data)
preds <- predict(fit, newdata=out_data)
err <- out_data$y - preds
mse <- mean(err^2)
# Record the RMSE
cv_err[i] <- sqrt(mse)
}
cv_err
train_data = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
train_data
prop.table(table(train_data$fold))
matrix_cv = matrix(data=0,nrow=length(linksfun),
ncol = kfolds,
dimnames = list(linksfun))
matrix_cv
tcv_data = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
set.seed(10)
kfolds = 10
tcv_data = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
cv_err = rep(0, kfolds)
matrix_cv = matrix(data=0,nrow=length(linksfun),
ncol = kfolds,
dimnames = list(linksfun))
for(link in 1:length(linksfun)){
for(i in 1:kfolds){
t_data = filter(tcv_data, fold!=i)
v_data = filter(tcv_data, fold==i)
fitbeta = betareg(tx_benbf ~.,
data=t_data,
link = linksfun[link])
predsbeta = predict(fitbeta, newdata=v_data)
err = v_data$tx_benbf - predsbeta
mse = mean(err^2)
# Record the RMSE
matrix_cv[link,i] <- sqrt(mse)
}
}
matrix_cv
matrix_cv
colMeans(matrix_cv)
min(colMeans(matrix_cv))
which(min(colMeans(matrix_cv)))
which(colMeans(matrix_cv)==min(colMeans(matrix_cv)))
linkmin=which(colMeans(matrix_cv)==min(colMeans(matrix_cv)))
matrix_cv[linkmin]
matrix_cv[linkmin,]
rownames(matrix_cv[linkmin,])
matrix_cv
matrix_cv ## ligação log-log é a que retorno o menor RMSE
modbeta = betareg(tx_benbf ~.,
data=treinobf,
link = "log-log")
modbeta = betareg(tx_benbf ~.,
data=treinobf,
link = "loglog")
ypbreg = predict(modbeta,testebf)
msebetareg = MSE(ypred=ypbreg,ytrue=testebf$tx_benbf)
msebetareg
mean((teste$tx_benbf - ypbreg)^2)
mean((testbf$tx_benbf - ypbreg)^2)
mean((testebf$tx_benbf - ypbreg)^2)
msebetareg = mean((testebf$tx_benbf - ypbreg)^2)
msebetareg
## RMSE
sqrt(msebetareg)
## MAE
mean(abs(testebf$tx_benbf - ypbreg))
R2 = function(y,ypred){
num = sum((y-ypred)^2)
dem =  sum((y-mean(y))^2)
r = 1 - (num/dem)
return(r)
}
R2(y=testebf$tx_benbf,ypred=ypbreg)
seq(100,1000,100)
seq(5,25,5)
3*3*3
#Tune the Random Forest model
install.packages("randomForest")
OptModelRF=tune(randomForest, tx_benbf~.,
data = treinobf,
ranges=list(ntree = c(500,500,1000),
mtry = c(5,10,25),
maxnodes = c(5,10,25)),
tunecontrol = tune.control(cross = 10))
## Dados: Taxa de beneficiários do bolsa família, em 2015.
## Bibliotecas
if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, tidyverse,
caret,   # for general data preparation and model fitting
e1071) # for fitting the xgboost model
## Leitura do banco de dados
setwd("C:/Users/dioni/OneDrive - University of São Paulo/Doutorado em Estatística/2023.2/2 - Aprendizado de Máquina Estatístico/Codigos_Pratica/3_Trabalho_Final/dados")
bf = read.csv("bolsafamilia_tx.csv")
head(bf)
## retirar a coluna indice
bf = bf %>% subset(select=-c(X))
## ---
## Separação das covariaveis e da resposta
## ---
head(bf)
y = bf %>% subset(select=c(tx_benbf))
X = bf %>% subset(select=c(FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,
GINI, PIND, PPOB, RDPCT,
THEIL, P_FORMAL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
PAREDE, T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R,
tx_ocupacao_urbana, capital))
databf = cbind(X,y)
dim(databf)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(databf) * prop_treino)
ind_treino = sample(1:nrow(databf), n_treino)
# Criar conjuntos de treino e teste
treinobf = databf[ind_treino, ]
testebf = databf[-ind_treino, ]
dim(treinobf);dim(testebf)
## -------
## Machine Learning Regression Models
## -------
## ----
## Máquinas de Vetores de Suporte (SVM)
## ----
## Tuning SVR model by varying values of maximum allowable error and cost parameter
#Tune the SVM model
OptModelsvm=tune(svm, tx_benbf~.,
data = treinobf,
ranges=list(epsilon=seq(0,1,0.1),
cost=1:10, kernel=c("linear", "polynomial",
"radial", "sigmoid"),
gamma=log(seq(1,10,0.9))),
tunecontrol = tune.control(cross = 10))
#Tune the Random Forest model
install.packages("randomForest")
OptModelRF=tune(randomForest, tx_benbf~.,
data = treinobf,
ranges=list(ntree = c(500,500,1000),
mtry = c(5,10,25),
maxnodes = c(5,10,25)),
tunecontrol = tune.control(cross = 10))
library(randomForest)
OptModelRF=tune(randomForest, tx_benbf~.,
data = treinobf,
ranges=list(ntree = c(500,500,1000),
mtry = c(5,10,25),
maxnodes = c(5,10,25)),
tunecontrol = tune.control(cross = 10))
OptModelRF=tune(randomForest, tx_benbf~.,
data = treinobf,
ranges=list(ntree = c(500,500,1000),
mtry = c(5,10,25),
maxnodes = c(5,10,25)),
tunecontrol = tune.control(cross = 5))
2*2
OptModelRF=tune(randomForest, tx_benbf~.,
data = treinobf,
ranges=list(ntree = c(500,1000),
mtry = c(5),
maxnodes = c(5,10)),
tunecontrol = tune.control(cross = 5))
1+1
#Print optimum value of parameters
print(OptModelRF)
#Plot the perfrormance of SVM Regression model
plot(OptModelRF)
seq(0,1,0.1)
seq(1,10,0.9))
seq(1,10,0.9))
seq(1,10,0.9)
seq(1,10,2))
seq(1,10,2)
log(seq(1,10,2))
seq(0.1,0.5,1)
1:10
seq(1,10,2
)
5*5*5
125*4
4*3*3
5*5*5
4*3*3
linksfun
6*5
kfolds = 10
tcv_bag = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
tcv_bag
set.seed(12)
b_values = c(100,200,500,1000,5000)
linksfun
kfolds = 10
tcv_bag = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
array(0, dim = c(length(b_values), length(linksfun), kfolds))
array_cv_bag = array(0, dim = c(length(b_values), length(linksfun), kfolds))
array_cv_bag
array_cv_bag = array(0, dim = c(length(linksfun), kfolds,length(b_values)))
array_cv_bag
bagging_betareg = function(xtreino,ytreino,xteste,n_estimadores,n_amostra,linkfun){
## predicoes do bagging em uma matriz
matriz_bag = matrix(data=0,nrow=dim(xteste)[1],ncol=n_estimadores)
for(preditor in 1:n_estimadores){
random_id = sample(1:dim(xtreino)[1],replace=T)
xtreino_bag = xtreino[random_id,]
ytreino_bag = ytreino[random_id]
dados_bag = cbind(ytreino_bag,xtreino_bag)
colnames(dados_bag)[1] = "y"
reg_bag = betareg(y ~ .,
data=dados_bag,
link=linkfun)
y_pred_bag = predict(reg_bag, newdata=xteste)
matriz_bag[,preditor] = y_pred_bag
}
predicoes_bag = rowSums(matriz_bag)/n_estimadores
return(predicoes_bag)
}
bagging_betareg(xtreino=treinobf[, -which(names(treinobf) == "tx_benbf")],
ytreino=treinobf[, "tx_benbf"],
xteste=testebf[, -which(names(testebf) == "tx_benbf")],
n_estimadores=100,
n_amostra=dim(treinobf)[1],
linkfun = "logit")
modbeta = betareg(tx_benbf ~.,
data=treinobf,
link = "loglog")
if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, betareg, betaboost,e1071,caret)
bf = read.csv("https://raw.githubusercontent.com/Dionisioneto/Aprendizado_de_Maquina_Estatistico/main/3_Trabalho_Final/dados/bolsafamilia_tx.csv")
head(bf)
## retirar a coluna indice
bf = bf %>% subset(select=-c(X))
y = bf %>% subset(select=c(tx_benbf))
X = bf %>% subset(select=c(FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,
GINI, PIND, PPOB, RDPCT,
THEIL, P_FORMAL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
PAREDE, T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R,
tx_ocupacao_urbana, capital))
y[y>0.99999] = 0.9999
databf = cbind(X,y)
dim(databf)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(databf) * prop_treino)
ind_treino = sample(1:nrow(databf), n_treino)
# Criar conjuntos de treino e teste
treinobf = databf[ind_treino, ]
testebf = databf[-ind_treino, ]
dim(treinobf);dim(testebf)
breg = betareg(tx_benbf ~.,
data=treinobf)
ypredbreg = predict(breg,testebf)
MSE = function(ypred,ytrue){sum((ypred - ytrue)^2)/length(ytrue)}
msebeta = MSE(ypred=ypredbreg,ytrue=testebf$tx_benbf)
msebeta
sqrt(msebeta)
linksfun = c("logit", "probit", "cloglog", "cauchit", "log", "loglog")
set.seed(10)
kfolds = 10
tcv_data = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
cv_err = rep(0, kfolds)
matrix_cv = matrix(data=0,nrow=length(linksfun),
ncol = kfolds,
dimnames = list(linksfun))
matrix_cv
colMeans(matrix_cv)
linkmin=which(colMeans(matrix_cv)==min(colMeans(matrix_cv)))
matrix_cv ## ligação log-log é a que retorno o menor RMSE
modbeta = betareg(tx_benbf ~.,
data=treinobf,
link = "loglog")
ypbreg = predict(modbeta,testebf)
msebetareg = mean((testebf$tx_benbf - ypbreg)^2)
msebetareg ## EQM
## RMSE
sqrt(msebetareg)
## MAE
mean(abs(testebf$tx_benbf - ypbreg))
R2 = function(y,ypred){
num = sum((y-ypred)^2)
dem =  sum((y-mean(y))^2)
r = 1 - (num/dem)
return(r)
}
## R2: Coeficiente de Determinação
R2(y=testebf$tx_benbf,ypred=ypbreg)
bagging_betareg = function(xtreino,ytreino,xteste,n_estimadores,n_amostra,linkfun){
## predicoes do bagging em uma matriz
matriz_bag = matrix(data=0,nrow=dim(xteste)[1],ncol=n_estimadores)
for(preditor in 1:n_estimadores){
random_id = sample(1:dim(xtreino)[1],replace=T)
xtreino_bag = xtreino[random_id,]
ytreino_bag = ytreino[random_id]
dados_bag = cbind(ytreino_bag,xtreino_bag)
colnames(dados_bag)[1] = "y"
reg_bag = betareg(y ~ .,
data=dados_bag,
link=linkfun)
y_pred_bag = predict(reg_bag, newdata=xteste)
matriz_bag[,preditor] = y_pred_bag
}
predicoes_bag = rowSums(matriz_bag)/n_estimadores
return(predicoes_bag)
}
y_pred_bagging= bagging_betareg(xtreino=treinobf[, -which(names(treinobf) == "tx_benbf")],
ytreino=treinobf[, "tx_benbf"],
xteste=testebf[, -which(names(testebf) == "tx_benbf")],
n_estimadores=100,
n_amostra=dim(treinobf)[1],
linkfun = "logit")
y_pred_bagging
tcv_data
array_cv_bag
array_cv_bag[1,,]
array_cv_bag
array_cv_bag
array_cv_bag[,,1]
1:length(b_values)
set.seed(12)
b_values = c(100,200,500,1000,5000)
linksfun
kfolds = 10
tcv_bag = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
array_cv_bag = array(0, dim = c(length(linksfun), kfolds,length(b_values)))
for (b in 1:length(b_values)){
for(link in 1:length(linksfun)){
for(i in 1:kfolds){
t_data = filter(tcv_data, fold!=i)
v_data = filter(tcv_data, fold==i)
predsbetabag = bagging_betareg(xtreino=t_data[, -which(names(t_data) == "tx_benbf")],
ytreino=t_data[, "tx_benbf"],
xteste=v_data[, -which(names(v_data) == "tx_benbf")],
n_estimadores=b_values[b],
n_amostra=dim(t_data)[1],
linkfun = linksfun[link])
errbag = v_data$tx_benbf - predsbetabag
msebag = mean(errbag^2)
# Record the RMSE
array_cv_bag[link,i,b] <- sqrt(mse)
}
}
}
b_values = c(100,200)
linksfun
kfolds = 10
tcv_bag = treinobf %>%
mutate(fold = sample(1:kfolds, size=dim(treinobf)[1], replace=T))
array_cv_bag = array(0, dim = c(length(linksfun), kfolds,length(b_values)))
for (b in 1:length(b_values)){
for(link in 1:length(linksfun)){
for(i in 1:kfolds){
t_data = filter(tcv_data, fold!=i)
v_data = filter(tcv_data, fold==i)
predsbetabag = bagging_betareg(xtreino=t_data[, -which(names(t_data) == "tx_benbf")],
ytreino=t_data[, "tx_benbf"],
xteste=v_data[, -which(names(v_data) == "tx_benbf")],
n_estimadores=b_values[b],
n_amostra=dim(t_data)[1],
linkfun = linksfun[link])
errbag = v_data$tx_benbf - predsbetabag
msebag = mean(errbag^2)
# Record the RMSE
array_cv_bag[link,i,b] <- sqrt(mse)
}
}
}
